# 机器学习
## 1.什么是机器学习？机器学习和深度学习有什么区别？
我认为，机器学习和深度学习都属于人工智能（AI）领域，是人工智能的核心，专门研究计算机怎样模拟或实现人类的学习行为。再往细划分，个人认为深度学习又属于机器学习。机器学习是通过算法及统计模型构建系统，能够根据实时更新的数据和经验，进行自我改进并应用于之后的计算及实验应用，它在一定程度上减轻了人工干涉的需要，减轻了程序员敲代码进行更新的负担。个人认为，深度学习与机器学习最大的不同在于深度学习的深度神经网络。深度神经网络是由多个层次（或称深度）的神经元组成的模型，与普通的机器学习相比，它的学习计算能力更强，能够处理更多，更复杂，更抽象的数据和信息。并且深度学习的显著特点是它的能力与处理的数据量成正相关，它在大规模的数据呵图像处理中表现出色，但通常也需要更多的计算和训练资源，而与之相比，机器学习所需的资源量就少得多。
## 2.请简述监督学习与无监督学习的概念，给出你对无监督学习和监督学习自己的理解和看法。
监督学习和无监督学习都是机器学习的两种范式，但它们有着显著的特征。
监督学习：机器依靠该范式进行学习时，参考及处理的数据来源于人工标记过的数据，这些数据包括输入的数据及经过机器处理后与输入数据相关的输出数据，监督学习的目标是让模型能够根据输入数据预测或分类新的未标记数据。
无监督学习：机器在该范式下学习时，处理的数据来源更加广泛，未被标记的数据也将经过机器处理加工并且输出，但其输出及目标并不明确。无监督学习的目标更倾向于发现数据间的联系，逻辑，结构，模式等。该范式下的算法并不依赖于标签，此范式下算法更倾向于在海量数据中提取真正有用的信息。
我的理解和看法：监督学习和无监督学习在机器学习中都发挥着重要作用，监督学习通常用于有实际标签的任务，在实际应用中能力十分强大，甚至可以通过大量被标记的数据在学习中预测未来的结果。而无监督学习在数据探索中则有着更明显的优势。它有助于发现数据其中包含潜在信息，但其结果通常需要进一步解释和分析。当二者在生产生活中相互结合使用，能够共同推动许多问题的解决。
## 3.偏导数是什么？链式法则是什么？梯度又是什么？矩阵乘法怎么操作？请仔细思考他们在机器学习/深度学习中的作用。
1.偏导数：偏导数适用于多元函数，是多元函数对其中一个变量的偏导数或变化率。偏导数告诉我们在一个多元函数中，一个自变量变化时，函数整体的变化率。例如，在函数f(x, y)中，偏导数∂f/∂x表示f关于x的变化率，而∂f/∂y表示f关于y的变化率。
2.链式法则：链式法则为微积分中的基本概念，用于计算复合函数的导数。例如，若有两个函数f(g(x))，则用链式法则表示该复合函数的导数为df/dx = (df/dg) * (dg/dx)。
3.梯度：梯度是一个向量，包含了一个多元函数中所有的偏导数。可以粗略地将梯度看作为多元函数每一个变量所含偏导数的集合。
4.矩阵乘法：若有两个矩阵A,B，其中元素分别为，b，且当A*B时，当A矩阵的列数等于B矩阵的行数时该乘法才有意义。具体操作为：假设矩阵C为A*B所得矩阵，则C中元素C（ij）等于所有a（ik）*b（kj）之和。其中k为正整数，且k最大值为A的列数（B的行数）。
**在机器学习/深度学习中的作用**：
（1）偏导数：在机器学习中，常见的损失函数通常是多元函数。我们通常需要计算关于多个参数的偏导数，以确定参数更新的方向。
（2）链式法则：神经网络通常是复合函数，链式法则能够让我们计算每一层神经网络中的差异，是实现自动求导的基础。
（3）梯度：梯度在训练神经网络时起着关键作用。可以计算损失函数关于模型参数的梯度，并使用梯度下降算法来优化模型参数。
（4）矩阵乘法：矩阵乘法用于定义神经网络中的权重矩阵和输入数据之间的关系。神经网络的前向传播和反向传播中都涉及到大量的矩阵乘法操作，它被高效地使用在现代深度学习框架中。
## 4.什么是损失函数？梯度下降的原理？反向传播的原理？
1.损失函数：损失函数是一个用于衡量模型预测与真实值之间差距的函数。它的值越小表示模型的预测越接近真实值。在监督学习中，损失函数通常是一个关于模型参数的函数，其目标是最小化损失函数的值。不同的任务和算法可以使用不同的损失函数。
2.梯度下降原理：梯度下降是一种优化算法，用于最小化损失函数。其基本原理是通过迭代更新模型参数，使损失函数逐渐减小。
3.反向传播的原理：它基于链式法则计算损失函数关于每个参数的梯度，并将梯度从输出层向输入层传播，以便更新网络中的权重。
## 5.什么是样本（sample）？什么是特征（feature）？为什么要使用激活函数？
1.样本：样本是指数据集中的一个单独的数据点或示例。在监督学习中，每个样本通常都有一个相关联的标签或目标值，用于表示我们希望模型学会的输出。例如，在图像分类任务中，一张图片可以是一个样本，而标签可以表示该图片所属的类别。
2.特征：特征是用于描述样本的属性或数据。特征可以是数字、文本、图像等不同类型的数据。在机器学习中，我们通常将每个样本表示为一个特征向量，其中每个特征对应向量中的一个维度。
3.为什么要使用激活函数：激活函数的引入是为了使神经网络具备非线性拟合能力。如果没有激活函数，多层神经网络将仅仅是一系列线性变换的组合，无论多少层，整个网络仍然等效于一个线性模型。这是因为多个线性变换的组合仍然是线性的。通过使用激活函数，神经元可以学会更复杂的映射，从而更好地捕捉数据中的非线性关系。这使得神经网络能够适应各种复杂的任务
## 6.请简述线性回归和逻辑回归的概念与基本原理。通过学习，总结出线性回归和逻辑回归的联系与区别。
1.线性回归：线性回归是一种用于建立输入变量（自变量）与连续输出变量（因变量）之间的线性关系的监督学习算法。它假设自变量和因变量之间存在一个线性关系，通过拟合一条最佳拟合直线（或平面，或超平面）来预测因变量的值。线性回归的目标是最小化残差平方和，即使得模型预测值与实际观测值之间的差异最小。
2.逻辑回归：逻辑回归是一种用于解决分类问题的监督学习算法。它通过建立自变量与类别之间的概率关系来进行分类。逻辑回归使用逻辑函数（sigmoid函数）将线性组合的输入映射到0到1之间的概率值，通常使用一个阈值来进行分类决策。
3.联系：线性回归和逻辑回归都基于线性模型，它们假设自变量与因变量之间存在线性关系，尝试找到最佳拟合线以描述数据，并且两者都使用最小化损失函数的方法来估计模型参数。
4.区别:线性回归通常用于解决回归问题，其中因变量是连续型的。而逻辑回归用于解决分类问题，其中因变量表示样本属于两个或多个类别中的一个。